{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Scrapping process\n",
    "\n",
    "Define needed imports and the different files paths\n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "SAVE_PATH = os.path.join(os.path.dirname(__file__), \"..\", \"data\", \"raw\")\n",
    "FILE_PATH = os.path.join(SAVE_PATH, \"funko.csv\")\n",
    "PRICE_LOG_PATH = os.path.join(SAVE_PATH, \"price_changes.csv\")"
   ],
   "id": "fbc121e30a2defb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "What I want from my scrape function?\n",
    "    - [] Check if the file directories exist\n",
    "    - [] Read any existing data if it exists\n",
    "    ### When fetching the data\n",
    "        - [] Have a limit for the amount of pages it retrieves\n",
    "        - [] Check if the response for each page is a success/failed\n",
    "        - [] Check if we ran to the end and have no more products returned\n",
    "        - [] Simple delay to not overload or set any alarm from the scrapping\n",
    "    ### Iterating the items\n",
    "        - [] Retrieve only the important info and append to the df\n",
    "    ### Saving the data\n",
    "        - [] Combined new data with existing one\n",
    "        - [] Drop duplicates\n",
    "        - [] Track price changes for existing items\n",
    "        - [] Save items and price changes to files"
   ],
   "id": "51f5b10610401170"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scrape_funko(base_url=\"https://funkoeurope.com/collections/all/products.json\",\n",
    "                 max_pages=20):\n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "    existing_df = pd.read_csv(FILE_PATH) if os.path.exists(FILE_PATH) else pd.DataFrame(columns=[\"name\", \"price\", \"image\", \"category\"])\n",
    "    price_log = pd.read_csv(PRICE_LOG_PATH) if os.path.exists(PRICE_LOG_PATH) else pd.DataFrame(columns=[\"name\", \"old_price\", \"new_price\", \"timestamp\"])\n",
    "\n",
    "    all_products = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        print(f\"Fetching: {url}\")\n",
    "        resp = requests.get(url)\n",
    "\n",
    "\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"‚ö†Ô∏è Failed to fetch page {page}\")\n",
    "            break\n",
    "\n",
    "        data = resp.json()\n",
    "        products = data.get(\"products\", [])\n",
    "\n",
    "        if not products:  # stop if no more products\n",
    "            print(\"No more products found, stopping pagination.\")\n",
    "            break\n",
    "\n",
    "        for product in products:\n",
    "            name = product[\"title\"]\n",
    "            vendor = product[\"vendor\"]\n",
    "            image = product[\"images\"][0][\"src\"] if product[\"images\"] else None\n",
    "            price = float(product[\"variants\"][0][\"price\"]) if product[\"variants\"] else None\n",
    "\n",
    "            all_products.append({\n",
    "                \"name\": name,\n",
    "                \"price\": price,\n",
    "                \"image\": image,\n",
    "                \"category\": vendor\n",
    "            })\n",
    "\n",
    "\n",
    "        time.sleep(random.uniform(5, 10))\n",
    "\n",
    "    new_df = pd.DataFrame(all_products)\n",
    "\n",
    "    # Merge with existing\n",
    "    combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    combined_df.drop_duplicates(subset=[\"name\"], keep=\"last\", inplace=True)\n",
    "\n",
    "    # Track price changes\n",
    "    for _, row in new_df.iterrows():\n",
    "        name, new_price = row[\"name\"], row[\"price\"]\n",
    "        old_entry = existing_df[existing_df[\"name\"] == name]\n",
    "        if not old_entry.empty:\n",
    "            old_price = float(old_entry.iloc[0][\"price\"])\n",
    "            if old_price != new_price:\n",
    "                print(f\"üí∞ Price change detected for {name}: {old_price} ‚Üí {new_price}\")\n",
    "                price_log = pd.concat([\n",
    "                    price_log,\n",
    "                    pd.DataFrame([{\n",
    "                        \"name\": name,\n",
    "                        \"old_price\": old_price,\n",
    "                        \"new_price\": new_price,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }])\n",
    "                ], ignore_index=True)\n",
    "\n",
    "    # Save updated datasets\n",
    "    combined_df.to_csv(FILE_PATH, index=False)\n",
    "    price_log.to_csv(PRICE_LOG_PATH, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved {len(combined_df)} products to {FILE_PATH}\")\n",
    "    print(f\"üìì Price log updated: {len(price_log)} entries total\")\n",
    "\n",
    "    return combined_df, price_log"
   ],
   "id": "9340e17fb10e92d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Main",
   "id": "a90de2e6d86d19b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df, price_changes = scrape_funko()\n",
    "    print(df.head())"
   ],
   "id": "1791ff9a2b890bc9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
